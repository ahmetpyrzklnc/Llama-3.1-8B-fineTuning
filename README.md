# Llama 3.1 8B Fine Tuning

 #### | General Status and Appearance
 ---

**Hello Everybody!**

***Technologies used in this project***

<br>

<p align="left">
    &emsp; 
      <img alt="Jupyter" width = 70; src="https://raw.githubusercontent.com/devicons/devicon/6910f0503efdd315c8f9b858234310c06e04d9c0/icons/jupyter/jupyter-original.svg">
    </a>
    &emsp; 
      <img alt="python" width = 80;  src="https://raw.githubusercontent.com/devicons/devicon/6910f0503efdd315c8f9b858234310c06e04d9c0/icons/python/python-original.svg">
    </a>
    &emsp;
    </a>
</p>

<br>

In this project, the 8B parameter version of the Llama 3.1 model developed by Meta was fine-tuned.
<br>


You can access the blog post for this code [here](https://medium.com/@ahmtklnc.software/llama-3-1-8byi-fine-tune-etmek-ad%C4%B1m-ad%C4%B1m-k%C4%B1lavuz-3fdc532e5fef)

```git

https://github.com/ahmetpyrzklnc/Llama-3.1-8B-fineTuning.git

````

:mailbox: You can reach me [here!](https://ahmetpyrzklnc.github.io/index.html)
